{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Generation\n",
    "\n",
    "In this exercise, you will design an RNN to generate baby names! You will design an RNN to learn to predict the next letter of a name given the preceding letters. This is a character-level RNN rather than a word-level RNN.\n",
    "\n",
    "This idea comes from this excellent blog post: http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from keras.layers import LSTM, SimpleRNN, GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data\n",
    "\n",
    "The training data we will use comes from this corpus:\n",
    "http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/\n",
    "\n",
    "Take a look at the training data in `data/names.txt`, which includes both boy and girl names. Below we load the file and convert it to all lower-case for simplicity.\n",
    "\n",
    "Note that we also add a special \"end\" character (in this case a period) to allow the model to learn to predict the end of a name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7939 names\n"
     ]
    }
   ],
   "source": [
    "with open('../data/names.txt') as f:\n",
    "    names = f.readlines()\n",
    "    names = [name.lower().strip() + '.' for name in names]\n",
    "\n",
    "print('Loaded %d names' % len(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aamir.',\n",
       " 'aaron.',\n",
       " 'abbey.',\n",
       " 'abbie.',\n",
       " 'abbot.',\n",
       " 'abbott.',\n",
       " 'abby.',\n",
       " 'abdel.',\n",
       " 'abdul.',\n",
       " 'abdulkarim.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to count all of the characters in our \"vocabulary\" and build a dictionary that translates between the character and its assigned index (and vice versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 28\n"
     ]
    }
   ],
   "source": [
    "chars = set()\n",
    "for name in names:\n",
    "    chars.update(name)\n",
    "vocab_size = len(chars)\n",
    "print('Vocabulary size:', vocab_size)\n",
    "\n",
    "char_inds = dict((c, i) for i, c in enumerate(chars))\n",
    "inds_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'k': 0,\n",
       " 'h': 1,\n",
       " 'z': 2,\n",
       " 'n': 3,\n",
       " 'i': 4,\n",
       " 'm': 5,\n",
       " 'a': 6,\n",
       " 'q': 7,\n",
       " '.': 8,\n",
       " '-': 9,\n",
       " 'v': 10,\n",
       " 'e': 11,\n",
       " 'f': 12,\n",
       " 'x': 13,\n",
       " 'g': 14,\n",
       " 's': 15,\n",
       " 'j': 16,\n",
       " 'd': 17,\n",
       " 'b': 18,\n",
       " 'r': 19,\n",
       " 'c': 20,\n",
       " 'p': 21,\n",
       " 'w': 22,\n",
       " 'o': 23,\n",
       " 't': 24,\n",
       " 'y': 25,\n",
       " 'u': 26,\n",
       " 'l': 27}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'k',\n",
       " 1: 'h',\n",
       " 2: 'z',\n",
       " 3: 'n',\n",
       " 4: 'i',\n",
       " 5: 'm',\n",
       " 6: 'a',\n",
       " 7: 'q',\n",
       " 8: '.',\n",
       " 9: '-',\n",
       " 10: 'v',\n",
       " 11: 'e',\n",
       " 12: 'f',\n",
       " 13: 'x',\n",
       " 14: 'g',\n",
       " 15: 's',\n",
       " 16: 'j',\n",
       " 17: 'd',\n",
       " 18: 'b',\n",
       " 19: 'r',\n",
       " 20: 'c',\n",
       " 21: 'p',\n",
       " 22: 'w',\n",
       " 23: 'o',\n",
       " 24: 't',\n",
       " 25: 'y',\n",
       " 26: 'u',\n",
       " 27: 'l'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inds_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1 - translate chars to indexes\n",
    "\n",
    "Most of the work of preparing the data is taken care of, but it is important to know the steps because they will be needed anytime you want to train an RNN. Use the dictionary created above to translate each example in `names` to its number format in `int_names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Translate names to their number format in int_names\n",
    "int_names = [[char_inds[x] for x in name] for name in names]\n",
    "# TODO: review compound list comprehension like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `create_matrix_from_sequences` will take the examples and create training data by cutting up names into input sequence of length `maxlen` and training labels, which are the following character. Make sure you understand this procedure because it is what will actually go into the network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 32016 name segments\n"
     ]
    }
   ],
   "source": [
    "def create_matrix_from_sequences(int_names, maxlen, step=1):\n",
    "    name_parts = []\n",
    "    next_chars = []\n",
    "    for name in int_names:\n",
    "        for i in range(0, len(name) - maxlen, step):\n",
    "            name_parts.append(name[i: i + maxlen])\n",
    "            next_chars.append(name[i + maxlen])\n",
    "\n",
    "    return name_parts, next_chars\n",
    "\n",
    "maxlen = 3\n",
    "name_parts, next_chars = create_matrix_from_sequences(int_names, maxlen)\n",
    "print('Created %d name segments' % len(name_parts))\n",
    "# Created fixed-size inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = sequence.pad_sequences(name_parts, maxlen=maxlen)\n",
    "y_train = np_utils.to_categorical(next_chars, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32016, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  6,  5],\n",
       "       [ 6,  5,  4],\n",
       "       [ 5,  4, 19],\n",
       "       [ 6,  6, 19],\n",
       "       [ 6, 19, 23]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2 - design a model\n",
    "\n",
    "Design your model below. Like before, you will need to set up the embedding layer, the recurrent layer, a dense connection and a softmax to predict the next character.\n",
    "\n",
    "Fit the model by running at least 10 epochs. Later you will generate names with the model. Getting around 30% accuracy will usually result in decent generations. What is the accuracy you would expect for random guessing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What layers will this model need?\n",
    "# Embedding, because need to go from letters/dictionary size to whatever size we want\n",
    "# Or could go directly via tensor(?)\n",
    "# Then, LSTM\n",
    "# Then, output layer Dense with vocab size output\n",
    "# Then, Activation('softmax'), why softmax?\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: See solution for rest\n",
    "# More dropout details in day 5\n",
    "# Will it ever get to 99% accuracy?\n",
    "    # No, there are names with the first 3 letters.\n",
    "    # So, after 3 letters there are multiple possibilities.  We can only figure out most likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-0c3f46555a8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=32, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the model\n",
    "\n",
    "We can sample the model by feeding in a few letters and using the model's prediction for the next letter. Then we feed the model's prediction back in to get the next letter, etc.\n",
    "\n",
    "The `sample` function is a helper to allow you to adjust the diversity of the samples. You can read more [here](https://en.wikipedia.org/wiki/Softmax_function#Reinforcement_learning).\n",
    "\n",
    "Read the `gen_name` function to understand how the model is sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(p, diversity=1.0):\n",
    "    p1 = np.asarray(p).astype('float64')\n",
    "    p1 = np.log(p1) / diversity\n",
    "    e_p1 = np.exp(p1)\n",
    "    s = np.sum(e_p1)\n",
    "    p1 = e_p1 / s\n",
    "    return np.argmax(np.random.multinomial(1, p1, 1))\n",
    "\n",
    "\n",
    "def gen_name(seed, length=1, diversity=1.0, maxlen=3):\n",
    "    \"\"\"\n",
    "    seed - the start of the name to sample\n",
    "    length - the number of letters to sample; if None then samples\n",
    "        are generated until the model generates a '.' character\n",
    "    diversity - a knob to increase or decrease the randomness of the\n",
    "        samples; higher = more random, lower = closer to the model's\n",
    "        prediction\n",
    "    maxlen - the size of the model's input\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare input array\n",
    "    x = np.zeros((1, maxlen), dtype=int)\n",
    "\n",
    "    # Generate samples\n",
    "    out = seed\n",
    "    while length is None or len(out) < len(seed) + length:\n",
    "\n",
    "        # Add the last chars so far for the next input\n",
    "        for i, c in enumerate(out[-maxlen:]):\n",
    "            x[0, i] = char_inds[c]\n",
    "        \n",
    "        # Get softmax for next character\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        \n",
    "        # Sample the network output with diversity\n",
    "        c = sample(preds, diversity)\n",
    "        \n",
    "        # Choose to end if the model generated an end token\n",
    "        if c == char_inds['.']:\n",
    "            if length is None:\n",
    "                return out\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        # Build up output\n",
    "        out += inds_char[c]\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3 - sample the model\n",
    "\n",
    "Use the `gen_name` function above to sample some names from your model.\n",
    "\n",
    "1. Try generating a few characters by setting the `length` argument.\n",
    "2. Try different diversities. Start with 1.0 and vary it up and down.\n",
    "3. Try using `length=None`, allowing the model to choose when to end a name.\n",
    "4. What happens when `length=None` and the diversity is high? How do samples change in this case staring from beginning to end? Why do you think this is?\n",
    "5. With `length=None` and a \"good\" diversity, can you tell if the model has learned a repertoire of \"endings\"? What are some of them? \n",
    "6. Find some good names. What are you favorites? :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not great names, but pronoucible, not necessarily English names.\n",
    "# Untrained model producses unpronouncible names\n",
    "# Why is this useful? Given enough data, we might be able to produce actually useful things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4 - retrain\n",
    "\n",
    "Now that you have seen some samples, go back up and redefine your model to \"erase\" it. Don't train it again yet. You can sample again to compare the quality of the samples before the model is trained.\n",
    "\n",
    "Experiment with the hidden layer size, the maxlen, the number of epochs, etc. Do you observe any differences in the sample behavior?\n",
    "\n",
    "Not all changes will make an observable impact, but do experiments to see what you can discover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
