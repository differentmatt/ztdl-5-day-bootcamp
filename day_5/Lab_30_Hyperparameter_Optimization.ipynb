{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats.distributions import expon, uniform, randint\n",
    "from sklearn.model_selection import train_test_split, ParameterSampler\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a couple of helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_dict(d):\n",
    "    for k, v in d.items():\n",
    "        print('  {:>20}: {}'.format(k, v))\n",
    "        \n",
    "def print_header(s):\n",
    "    divider = '=' * (len(s) + 4)\n",
    "    print()\n",
    "    print(divider)\n",
    "    print('  {}  '.format(s))\n",
    "    print(divider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train_valid, y_train_valid), (X_test, y_test) = cifar10.load_data()\n",
    "X_train_valid = X_train_valid.astype('float32') / 255.\n",
    "X_test = X_test.astype('float32') / 255.\n",
    "\n",
    "y_train_valid = to_categorical(y_train_valid)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_valid, y_train_valid, test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shapes: x = (45000, 32, 32, 3), y = (45000, 10)\n",
      "Valid shapes: x = (5000, 32, 32, 3), y = (5000, 10)\n",
      "Test  shapes: x = (10000, 32, 32, 3), y = (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print('Train shapes: x = {}, y = {}'.format(\n",
    "    X_train.shape, y_train.shape))\n",
    "print('Valid shapes: x = {}, y = {}'.format(\n",
    "    X_valid.shape, y_valid.shape))\n",
    "print('Test  shapes: x = {}, y = {}'.format(\n",
    "    X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation\n",
    "Make a function which accepts a config object containing your hyperparameters and returns a compiled model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_compile(config):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # first convolution / pooling set\n",
    "    model.add(Conv2D(config.conv1_num_filters, \n",
    "                     config.conv_filter_size, \n",
    "                     activation=config.activation, \n",
    "                     padding='same',\n",
    "                     input_shape=X_train.shape[1:]))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # second convolution / pooling set\n",
    "    model.add(Conv2D(config.conv2_num_filters, \n",
    "                     config.conv_filter_size, \n",
    "                     padding='same',\n",
    "                     activation=config.activation))\n",
    "    model.add(Conv2D(config.conv3_num_filters, \n",
    "                     config.conv_filter_size, \n",
    "                     padding='same',\n",
    "                     activation=config.activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # third convolution / pooling set\n",
    "    model.add(Conv2D(config.conv4_num_filters, \n",
    "                     config.conv_filter_size, \n",
    "                     padding='same',\n",
    "                     activation=config.activation))\n",
    "    model.add(Conv2D(config.conv5_num_filters, \n",
    "                     config.conv_filter_size, \n",
    "                     padding='same',\n",
    "                     activation=config.activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(config.dense1_size,\n",
    "                    activation=config.activation))\n",
    "    model.add(Dropout(config.dropout))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(lr=config.learn_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Selection\n",
    "Define the legal ranges for your hyperparameters and use `Sklearn`'s `ParameterSampler` to sample hyperparameters sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# powers of 2 tend to be good spreads\n",
    "# learn_rate tends to be most important, need to get within 2x of optimal.\n",
    "#     too high, and you step out (vaselate sp?) of your loss valley, too low and you learn too slowly\n",
    "#     generally give a big spread of learn_rate, kitchen sink time\n",
    "# How do we know which params?\n",
    "# Also, batch_size and learn_rate correlated, batch up learn goes up\n",
    "# Have to explore the space, very hard to manually guess correctly at the same level of efficiency\n",
    "# Lots of trial and error\n",
    "# TODO: how do you know which params are important?\n",
    "hp_ranges = {\n",
    "    'conv1_num_filters': [32, 64, 128],\n",
    "    'conv2_num_filters': [32, 64, 128],\n",
    "    'conv3_num_filters': [32, 64, 128],\n",
    "    'conv4_num_filters': [32, 64, 128],\n",
    "    'conv5_num_filters': [32, 64, 128],\n",
    "    'dense1_size':       [32, 64, 128, 256, 512],\n",
    "    'dropout':           uniform, # comes from sklearn above, random num gen?\n",
    "    'learn_rate':        [0.1, 0.03, 0.001],\n",
    "    'batch_size':        [8, 16, 32, 64, 128],\n",
    "}\n",
    "\n",
    "hp_sets = ParameterSampler(hp_ranges, n_iter=2, random_state=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyperparameter Set 0:\n",
      "            batch_size: 16\n",
      "     conv1_num_filters: 64\n",
      "     conv2_num_filters: 32\n",
      "     conv3_num_filters: 64\n",
      "     conv4_num_filters: 32\n",
      "     conv5_num_filters: 128\n",
      "           dense1_size: 512\n",
      "               dropout: 0.8080499633648477\n",
      "            learn_rate: 0.03\n",
      "\n",
      "Hyperparameter Set 1:\n",
      "            batch_size: 64\n",
      "     conv1_num_filters: 64\n",
      "     conv2_num_filters: 64\n",
      "     conv3_num_filters: 32\n",
      "     conv4_num_filters: 64\n",
      "     conv5_num_filters: 128\n",
      "           dense1_size: 256\n",
      "               dropout: 0.7467671009942997\n",
      "            learn_rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "for i, hp_set in enumerate(hp_sets):\n",
    "    print()\n",
    "    print(\"Hyperparameter Set {}:\".format(i))\n",
    "    print_dict(hp_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Static hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "static_hyper_params = {\n",
    "    'activation': 'relu',\n",
    "    'conv_filter_size': 3,\n",
    "    'num_epochs': 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over `hp_sets`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR: No W&B project configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============================================\n",
      "  Starting Training for Hyperparameter Set 1:  \n",
      "===============================================\n",
      "W&B Run: https://app.wandb.ai/mattlott/ztdl-lab30/runs/sc1gna3q\n",
      "Call `%%wandb` in the cell containing your training loop to display live results.\n",
      "            batch_size: 16\n",
      "     conv1_num_filters: 64\n",
      "     conv2_num_filters: 32\n",
      "     conv3_num_filters: 64\n",
      "     conv4_num_filters: 32\n",
      "     conv5_num_filters: 128\n",
      "           dense1_size: 512\n",
      "               dropout: 0.8080499633648477\n",
      "            learn_rate: 0.03\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 32)        18464     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 32)          18464     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 128)         36992     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 1,148,426\n",
      "Trainable params: 1,148,426\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "45000/45000 [==============================] - 39s 861us/step - loss: 14.4906 - acc: 0.1007 - val_loss: 14.5772 - val_acc: 0.0956\n",
      "Resuming run: https://app.wandb.ai/mattlott/ztdl-lab30/runs/sc1gna3q\n",
      "Epoch 2/2\n",
      "45000/45000 [==============================] - 42s 935us/step - loss: 14.4995 - acc: 0.1004 - val_loss: 14.5772 - val_acc: 0.0956\n",
      "\n",
      "===============================================\n",
      "  Starting Training for Hyperparameter Set 1:  \n",
      "===============================================\n",
      "W&B Run: https://app.wandb.ai/mattlott/ztdl-lab30/runs/7fqzobxq\n",
      "Call `%%wandb` in the cell containing your training loop to display live results.\n",
      "            batch_size: 64\n",
      "     conv1_num_filters: 64\n",
      "     conv2_num_filters: 64\n",
      "     conv3_num_filters: 32\n",
      "     conv4_num_filters: 64\n",
      "     conv5_num_filters: 128\n",
      "           dense1_size: 256\n",
      "               dropout: 0.7467671009942997\n",
      "            learn_rate: 0.001\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 16, 16, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 8, 8, 64)          18496     \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               524544    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 676,650\n",
      "Trainable params: 676,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "45000/45000 [==============================] - 19s 411us/step - loss: 1.8516 - acc: 0.3038 - val_loss: 1.5627 - val_acc: 0.4172\n",
      "Resuming run: https://app.wandb.ai/mattlott/ztdl-lab30/runs/7fqzobxq\n",
      "Epoch 2/2\n",
      "45000/45000 [==============================] - 19s 414us/step - loss: 1.4374 - acc: 0.4723 - val_loss: 1.2157 - val_acc: 0.5646\n"
     ]
    }
   ],
   "source": [
    "best_valid_acc = 0.0\n",
    "best_hp_set = None\n",
    "best_hp_ind = None\n",
    "\n",
    "for hp_ind, hp_set in enumerate(hp_sets):\n",
    "    # set up wandb\n",
    "    print_header(\"Starting Training for Hyperparameter Set {}:\".format(i))\n",
    "    wandb.init()\n",
    "    ## For short runs like this, wandb.monitor()\n",
    "    # is just visual noise.  Reenable it for longer runs.\n",
    "    # wandb.monitor()\n",
    "    print_dict(hp_set)\n",
    "   \n",
    "    wandb.config.update(static_hyper_params, allow_val_change=True)\n",
    "    wandb.config.update(hp_set, allow_val_change=True)\n",
    "\n",
    "    # build model\n",
    "    model = build_compile(wandb.config)\n",
    "    print(model.summary())\n",
    "    wandb.config.num_model_parameters = model.count_params()\n",
    "    \n",
    "    # train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=wandb.config.batch_size,\n",
    "        epochs=wandb.config.num_epochs,\n",
    "        verbose=1,\n",
    "        validation_data=(X_valid, y_valid),\n",
    "        callbacks=[WandbCallback()]\n",
    "    )\n",
    "    \n",
    "    # track best model so far\n",
    "    valid_acc = history.history['val_acc'][-1]\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        best_hp_set = hp_set\n",
    "        best_hp_ind = hp_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain Best Model on Full train+valid Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================\n",
      "  Best Hyperparams were set 1 with valid accuracy 0.5646  \n",
      "==========================================================\n",
      "            batch_size: 64\n",
      "     conv1_num_filters: 64\n",
      "     conv2_num_filters: 64\n",
      "     conv3_num_filters: 32\n",
      "     conv4_num_filters: 64\n",
      "     conv5_num_filters: 128\n",
      "           dense1_size: 256\n",
      "               dropout: 0.7467671009942997\n",
      "            learn_rate: 0.001\n",
      "Epoch 1/2\n",
      "50000/50000 [==============================] - 20s 392us/step - loss: 1.7780 - acc: 0.3401\n",
      "Resuming run: https://app.wandb.ai/mattlott/ztdl-lab30/runs/7fqzobxq\n",
      "Epoch 2/2\n",
      "50000/50000 [==============================] - 19s 370us/step - loss: 1.3655 - acc: 0.5127\n"
     ]
    }
   ],
   "source": [
    "print_header(\"Best Hyperparams were set {} with valid accuracy {}\".format(best_hp_ind, best_valid_acc))\n",
    "print_dict(best_hp_set)\n",
    "\n",
    "# Retrain model on combined training and validation data\n",
    "wandb.config.update(best_hp_set)\n",
    "model = build_compile(wandb.config)\n",
    "history = model.fit(\n",
    "    X_train_valid, y_train_valid,\n",
    "    batch_size=wandb.config.batch_size,\n",
    "    epochs=wandb.config.num_epochs,\n",
    "    verbose=1,\n",
    "    callbacks=[WandbCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 128us/step\n",
      "Test loss: 1.2283068012237548, test acc: 0.5685\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test,\n",
    "                           batch_size=wandb.config.batch_size)\n",
    "print(\"Test loss: {}, test acc: {}\".format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Results on WandB \n",
    "Go to https://app.wandb.ai/, then select your project name to see a summary of all your runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Gotchas\n",
    "* It's easy to accidentally explode the size of your model.  In particular you get lots of parameters when:\n",
    "  * You don't use much MaxPooling\n",
    "  * You have a large first Dense layer after you Conv layers.\n",
    "* As batch size goes up, learning rate can go up.  As batch size goes down, learning rate must go down.  Why?\n",
    "  * More data leads to better gradient estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1:\n",
    "* Create a function, `build_compile_ex1`, which can create a CNN with a variable number of convolutional and dense layers using the hyperparameter ranges below.\n",
    "  * Remember that you'll need to special case the first conv layer to set `input_shape`.\n",
    "  * The hyperparameter `num_convs_per_max_pool` chooses how many conv layers should pass between each max pooling layer. \n",
    "    * You'll probably find python's modulus division operator useful for this.  e.g.: `5 % 3 ==> 2; 6 % 3 ==> 0`\n",
    "* Use the hyperparameter sets in `hp_sets_ex1` as your hyperparameter samples.\n",
    "* The number of filters in each conv layer can be constant, the number of neurons in the dense layer should be constant.\n",
    "* Include a `Dropout` layer after each `Dense` layer.\n",
    "* Don't forget the `Flatten` layer before switching to `Dense`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Legal Hyperparameter Ranges\n",
    "hp_ranges_ex1 = {\n",
    "    'num_conv_filters':       [32, 64, 128],\n",
    "    'num_conv_layers':        randint(2, 8),\n",
    "    'num_convs_per_max_pool': randint(1, 3),\n",
    "    'dense_size':             [32, 64, 128, 256, 512],\n",
    "    'num_dense_layers':       randint(1, 3),\n",
    "    'dropout':                uniform,\n",
    "    'learn_rate':             [0.1, 0.03, 0.001],\n",
    "    'batch_size':             [8, 16, 32, 64, 128],\n",
    "}\n",
    "\n",
    "hp_sets_ex1 = ParameterSampler(hp_ranges_ex1, n_iter=2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyperparameter Set 0:\n",
      "            batch_size: 64\n",
      "            dense_size: 512\n",
      "               dropout: 0.6121118946772708\n",
      "            learn_rate: 0.03\n",
      "      num_conv_filters: 64\n",
      "       num_conv_layers: 3\n",
      "  num_convs_per_max_pool: 1\n",
      "      num_dense_layers: 1\n",
      "\n",
      "Hyperparameter Set 1:\n",
      "            batch_size: 64\n",
      "            dense_size: 512\n",
      "               dropout: 0.15881535318443651\n",
      "            learn_rate: 0.001\n",
      "      num_conv_filters: 128\n",
      "       num_conv_layers: 2\n",
      "  num_convs_per_max_pool: 1\n",
      "      num_dense_layers: 1\n"
     ]
    }
   ],
   "source": [
    "for i, hp_set in enumerate(hp_sets_ex1):\n",
    "    print()\n",
    "    print(\"Hyperparameter Set {}:\".format(i))\n",
    "    print_dict(hp_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your `build_compile_ex1` function in the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "tags": [
     "solution",
     "empty"
    ]
   },
   "outputs": [],
   "source": [
    "def build_compile_ex1(config):\n",
    "    model = Sequential()\n",
    "\n",
    "    ######### YOUR CODE HERE #########\n",
    "\n",
    "    for i in range(config.num_conv_layers):\n",
    "        if i is 0:\n",
    "            model.add(Conv2D(config.num_conv_filters, \n",
    "                             config.conv_filter_size, \n",
    "                             padding='same',\n",
    "                             activation=config.activation,\n",
    "                             input_shape=X_train.shape[1:]))\n",
    "        else:\n",
    "            model.add(Conv2D(config.num_conv_filters, \n",
    "                             config.conv_filter_size, \n",
    "                             padding='same',\n",
    "                             activation=config.activation))\n",
    "#         if config.num_convs_per_max_pool % (i+1) is 0:\n",
    "# From TA solution:\n",
    "# Example of down sampling\n",
    "        if not i % config.num_convs_per_max_pool:\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "#     Key to back propagation and loss function\n",
    "    for i in range(config.num_dense_layers):\n",
    "        model.add(Dense(config.dense_size,\n",
    "                        activation=config.activation))\n",
    "        model.add(Dropout(config.dropout))\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(lr=config.learn_rate),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============================================\n",
      "  Starting Training for Hyperparameter Set 1:  \n",
      "===============================================\n",
      "W&B Run: https://app.wandb.ai/mattlott/ztdl-lab30/runs/vw7m42xv\n",
      "Call `%%wandb` in the cell containing your training loop to display live results.\n",
      "            batch_size: 64\n",
      "            dense_size: 512\n",
      "               dropout: 0.6121118946772708\n",
      "            learn_rate: 0.03\n",
      "      num_conv_filters: 64\n",
      "       num_conv_layers: 3\n",
      "  num_convs_per_max_pool: 1\n",
      "      num_dense_layers: 1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_29 (Conv2D)           (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 605,578\n",
      "Trainable params: 605,578\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "45000/45000 [==============================] - 13s 280us/step - loss: 14.4805 - acc: 0.1006 - val_loss: 14.5514 - val_acc: 0.0972\n",
      "Resuming run: https://app.wandb.ai/mattlott/ztdl-lab30/runs/vw7m42xv\n",
      "Epoch 2/2\n",
      "45000/45000 [==============================] - 13s 281us/step - loss: 14.4991 - acc: 0.1004 - val_loss: 14.5514 - val_acc: 0.0972\n",
      "\n",
      "===============================================\n",
      "  Starting Training for Hyperparameter Set 1:  \n",
      "===============================================\n",
      "W&B Run: https://app.wandb.ai/mattlott/ztdl-lab30/runs/9w3ibjnm\n",
      "Call `%%wandb` in the cell containing your training loop to display live results.\n",
      "            batch_size: 64\n",
      "            dense_size: 512\n",
      "               dropout: 0.15881535318443651\n",
      "            learn_rate: 0.001\n",
      "      num_conv_filters: 128\n",
      "       num_conv_layers: 2\n",
      "  num_convs_per_max_pool: 1\n",
      "      num_dense_layers: 1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_32 (Conv2D)           (None, 32, 32, 128)       3584      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 4,351,114\n",
      "Trainable params: 4,351,114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "45000/45000 [==============================] - 18s 389us/step - loss: 1.4124 - acc: 0.4919 - val_loss: 1.1354 - val_acc: 0.5918\n",
      "Resuming run: https://app.wandb.ai/mattlott/ztdl-lab30/runs/9w3ibjnm\n",
      "Epoch 2/2\n",
      "45000/45000 [==============================] - 17s 367us/step - loss: 1.0180 - acc: 0.6406 - val_loss: 0.9567 - val_acc: 0.6568\n"
     ]
    }
   ],
   "source": [
    "static_hyper_params = {\n",
    "    'activation': 'relu',\n",
    "    'conv_filter_size': 3,\n",
    "    'num_epochs': 2,\n",
    "}\n",
    "\n",
    "best_valid_acc = 0.0\n",
    "best_hp_set = None\n",
    "best_hp_ind = None\n",
    "\n",
    "for hp_ind, hp_set in enumerate(hp_sets_ex1):\n",
    "    # set up wandb\n",
    "    print_header(\"Starting Training for Hyperparameter Set {}:\".format(i))\n",
    "    wandb.init()\n",
    "    ## For short runs like this, wandb.monitor()\n",
    "    # is just visual noise.  Reenable it for longer runs.\n",
    "    # wandb.monitor()\n",
    "    print_dict(hp_set)\n",
    "   \n",
    "    wandb.config.update(static_hyper_params, allow_val_change=True)\n",
    "    wandb.config.update(hp_set, allow_val_change=True)\n",
    "\n",
    "    # build model\n",
    "    model = build_compile_ex1(wandb.config)\n",
    "    print(model.summary())\n",
    "    wandb.config.num_model_parameters = model.count_params()\n",
    "    \n",
    "    # train model \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=wandb.config.batch_size,\n",
    "        epochs=wandb.config.num_epochs,\n",
    "        verbose=1,\n",
    "        validation_data=(X_valid, y_valid),\n",
    "        callbacks=[WandbCallback()]\n",
    "    )\n",
    "    \n",
    "    valid_acc = history.history['val_acc'][-1]\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        best_hp_set = hp_set\n",
    "        best_hp_ind = hp_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================\n",
      "  Best Hyperparams were set 1 with valid accuracy 0.6568  \n",
      "==========================================================\n",
      "            batch_size: 64\n",
      "            dense_size: 512\n",
      "               dropout: 0.15881535318443651\n",
      "            learn_rate: 0.001\n",
      "      num_conv_filters: 128\n",
      "       num_conv_layers: 2\n",
      "  num_convs_per_max_pool: 1\n",
      "      num_dense_layers: 1\n",
      "Epoch 1/2\n",
      "50000/50000 [==============================] - 19s 374us/step - loss: 1.3879 - acc: 0.5013\n",
      "Resuming run: https://app.wandb.ai/mattlott/ztdl-lab30/runs/9w3ibjnm\n",
      "Epoch 2/2\n",
      "50000/50000 [==============================] - 17s 349us/step - loss: 0.9960 - acc: 0.6495\n"
     ]
    }
   ],
   "source": [
    "print_header(\"Best Hyperparams were set {} with valid accuracy {}\".format(best_hp_ind, best_valid_acc))\n",
    "print_dict(best_hp_set)\n",
    "\n",
    "# Retrain model on combined training and validation data\n",
    "wandb.config.update(best_hp_set)\n",
    "model = build_compile_ex1(wandb.config)\n",
    "history = model.fit(\n",
    "    X_train_valid, y_train_valid,\n",
    "    batch_size=wandb.config.batch_size,\n",
    "    epochs=wandb.config.num_epochs,\n",
    "    verbose=1,\n",
    "    callbacks=[WandbCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 170us/step\n",
      "Test loss: 0.9220220293045044, test acc: 0.6816\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test, batch_size=wandb.config.batch_size)\n",
    "print(\"Test loss: {}, test acc: {}\".format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "* In practice, you don't conduct a hyperparameter search by wrapping many training runs in a for loop on a single machine.  \n",
    "* Instead, you want to have a single machine which selects the hyperparameter sets, then sends them off to worker nodes which actually conduct the training.\n",
    "* Multi-node training isn't hard to do, but it's out of scope for this 1-week class; too many IT hurdles.  In this exercise, though, we'll refactor our existing code to more closely approximate a real training setup.\n",
    "\n",
    "### Instructions\n",
    "* Refactor your existing code into a script rather than a notebook.\n",
    "* The script should accept a series of keyword arguments containing all the hyperparameter values for a single run.  Check out the `argparse` python module.\n",
    "* It should then combine these arguments into a Python dict representing a single hyperparameter set like the `hp_set` variable above.\n",
    "* The script should then update the wandb.config object with the values from the input hyperparameter set and train a model using those values.  You don't need to save the final result anywhere, the `WandbCallback()` will take care of that for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Too much devops, left as homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "*  Create a large number of hyperparameter sets.\n",
    "*  For each hyperparameter set, print out the model summary and study the number of parameters that are produced.  Try to get a sense for what configurations produce large parameter counts.\n",
    "*  If you have time, train models based on some of these hyperparameter sets and see which produce good results and which don't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Num parameters = overfit more a problem, need more data.  consumes more GPU memory => slower\n",
    "hp_sets_ex3 = ParameterSampler(hp_ranges_ex1, n_iter=100, random_state=1234)\n",
    "len(hp_sets_ex3)\n",
    "\n",
    "# for hp_ind, hp_set in enumerate(hp_sets_ex3):\n",
    "#     # set up wandb\n",
    "#     print_header(\"Starting Training for Hyperparameter Set {}:\".format(i))\n",
    "#     wandb.init()\n",
    "#     ## For short runs like this, wandb.monitor()\n",
    "#     # is just visual noise.  Reenable it for longer runs.\n",
    "#     # wandb.monitor()\n",
    "#     print_dict(hp_set)\n",
    "   \n",
    "#     wandb.config.update(static_hyper_params, allow_val_change=True)\n",
    "#     wandb.config.update(hp_set, allow_val_change=True)\n",
    "\n",
    "#     # build model\n",
    "#     model = build_compile_ex1(wandb.config)\n",
    "#     print(model.summary())\n",
    "#     wandb.config.num_model_parameters = model.count_params()\n",
    "    \n",
    "#     # train model \n",
    "#     history = model.fit(\n",
    "#         X_train, y_train,\n",
    "#         batch_size=wandb.config.batch_size,\n",
    "#         epochs=wandb.config.num_epochs,\n",
    "#         verbose=1,\n",
    "#         validation_data=(X_valid, y_valid),\n",
    "#         callbacks=[WandbCallback()]\n",
    "#     )\n",
    "    \n",
    "#     valid_acc = history.history['val_acc'][-1]\n",
    "#     if valid_acc > best_valid_acc:\n",
    "#         best_valid_acc = valid_acc\n",
    "#         best_hp_set = hp_set\n",
    "#         best_hp_ind = hp_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
